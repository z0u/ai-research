{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modal test\n",
    "\n",
    "This notebook uses Modal to run code remotely. Before running this notebook, you need to authenticate:\n",
    "\n",
    "```bash\n",
    "uv run modal setup\n",
    "```\n",
    "\n",
    "Then restart the notebook kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote function\n",
    "\n",
    "Let's define a mock training function that will run remotely. It just loops a few times and returns a stub model function.\n",
    "\n",
    "A [distributed `Queue`](https://modal.com/docs/reference/modal.Queue) is used to send progress information back. You can push rich data (like actual Matplotlib figures) onto the queue, and it transparently handles serialization - but let's keep it simple and leave the display to the caller.\n",
    "\n",
    "We specify the exact packages that we'll need in the image to keep it small. Version specifiers are needed (see [`freeze`](src/ai_eggs/requirements.py)), so that:\n",
    "- The remote function behaves exactly how it would locally\n",
    "- Objects can be pickled and sent back and forth.\n",
    "\n",
    "Since the queue is passed in as a parameter, we add `modal` itself as a dependency. In this example, we also add the local package `ai_eggs`. It's not strictly needed for the function to run, but Modal currently auto-mounts it and issues a warning about it, so we may as well be explicit that that's what is happening.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import modal\n",
    "\n",
    "from ai_eggs.requirements import freeze\n",
    "\n",
    "app = modal.App()\n",
    "\n",
    "\n",
    "@app.function(\n",
    "    image=(\n",
    "        modal.Image\n",
    "        .debian_slim()\n",
    "        # 'modal' is needed to unpickle the Queue\n",
    "        .pip_install(freeze('modal'))\n",
    "        .add_local_python_source('ai_eggs')\n",
    "    ),\n",
    "    # gpu=\"T4\",\n",
    ")\n",
    "async def train(epochs: int, comms):\n",
    "    for i in range(epochs):\n",
    "        comms.emit('metrics', {\"epoch\": i, \"loss\": 1/(i+1)})\n",
    "        sleep(0.5)\n",
    "\n",
    "    def stub_model(x): return f\"model({x})\"\n",
    "    return stub_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call and display progress locally\n",
    "\n",
    "Now let's run that code remotely.\n",
    "\n",
    "If we only cared about the final result, or if we were happy just printing progress to stdout, we could call `train` synchronously. But by calling it asynchronously with `.remote.aio(...)`, we can chart the metrics while it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import create_task, wait_for\n",
    "from contextlib import asynccontextmanager\n",
    "from typing import Any, Callable\n",
    "\n",
    "import modal\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Comms:\n",
    "    queue: modal.Queue\n",
    "\n",
    "    def __init__(self, queue: modal.Queue):\n",
    "        self.queue = queue\n",
    "\n",
    "    @classmethod\n",
    "    @asynccontextmanager\n",
    "    async def create(cls):\n",
    "        with modal.Queue.ephemeral() as queue:\n",
    "            yield cls(queue)\n",
    "\n",
    "    @asynccontextmanager\n",
    "    async def auto_close(self):\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "    def close(self):\n",
    "        self.queue.put(None)\n",
    "\n",
    "    def emit(self, key: str, value: Any):\n",
    "        self.queue.put((key, value))\n",
    "\n",
    "    @asynccontextmanager\n",
    "    async def subscribe(self, key: str, receive: Callable[[Any], None]):\n",
    "        async def consume():\n",
    "            while message := await self.queue.get.aio():\n",
    "                k, v = message\n",
    "                if k == key:\n",
    "                    receive(v)\n",
    "\n",
    "        task = create_task(consume())\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            task.cancel()\n",
    "            await wait_for(task, timeout=3)\n",
    "\n",
    "\n",
    "losses: list[float] = []\n",
    "\n",
    "\n",
    "def progress(message: dict[str, float | int]):\n",
    "    losses.append(message['loss'])\n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(range(len(losses)), losses)\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "async with app.run(), Comms.create() as comms:\n",
    "    async with comms.subscribe('metrics', progress), comms.auto_close():\n",
    "        model = await train.remote.aio(5, comms)\n",
    "\n",
    "print(model(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

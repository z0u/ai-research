{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modal test\n",
    "\n",
    "This notebook uses Modal to run code remotely. Before running this notebook, you need to authenticate:\n",
    "\n",
    "```bash\n",
    "uv run modal setup\n",
    "```\n",
    "\n",
    "Then restart the notebook kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote function\n",
    "\n",
    "Let's define a mock training function that will run remotely. It just loops a few times and returns a stub model function.\n",
    "\n",
    "A [distributed `Queue`](https://modal.com/docs/reference/modal.Queue) is used to send progress information back. You can push rich data (like actual Matplotlib figures) onto the queue, and it transparently handles serialization - but let's keep it simple and leave the display to the caller.\n",
    "\n",
    "We specify the exact packages that we'll need in the image to keep it small. Version specifiers are needed (see [`freeze`](src/ai_eggs/requirements.py)), so that:\n",
    "- The remote function behaves exactly how it would locally\n",
    "- Objects can be pickled and sent back and forth.\n",
    "\n",
    "Since the queue is passed in as a parameter, we add `modal` itself as a dependency. In this example, we also add the local package `ai_eggs`. It's not strictly needed for the function to run, but Modal currently auto-mounts it and issues a warning about it, so we may as well be explicit that that's what is happening.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from typing import Callable\n",
    "import modal\n",
    "\n",
    "from ai_eggs.requirements import freeze\n",
    "\n",
    "app = modal.App()\n",
    "\n",
    "\n",
    "@app.function(\n",
    "    image=(\n",
    "        modal.Image\n",
    "        .debian_slim()\n",
    "        # 'modal' is needed to unpickle the Queue\n",
    "        .pip_install(freeze('modal'))\n",
    "        .add_local_python_source('ai_eggs')\n",
    "    ),\n",
    "    # gpu=\"T4\",\n",
    ")\n",
    "def train(epochs: int, emit_metrics: Callable[[dict], None]):\n",
    "    for i in range(epochs):\n",
    "        emit_metrics({\"epoch\": i, \"loss\": 1/(i+1)})\n",
    "        sleep(0.5)\n",
    "\n",
    "    def stub_model(x):\n",
    "        if x == \"What is the meaning of life?\":\n",
    "            return \"Forty-two.\"\n",
    "        else:\n",
    "            return \"I don't know that!\"\n",
    "    return stub_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display progress locally\n",
    "\n",
    "Let's get some metrics displayed right here in the notebook! We'll define a function to chart metrics. This function will be called several times during training, and it should update the chart each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_loss(losses: list[float]):\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.set_title('Training progress')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ybound(0, 1)\n",
    "    ax.plot(range(len(losses)), losses, label='Loss')\n",
    "    ax.legend()\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def Progress():\n",
    "    losses: list[float] = []\n",
    "    # Use a display_id to update the plot without clearing the whole cell output\n",
    "    display_id = display(plot_loss(losses), display_id=True)\n",
    "\n",
    "    def progress(message: dict[str, float | int]):\n",
    "        losses.append(message['loss'])\n",
    "        display_id.update(plot_loss(losses))\n",
    "\n",
    "    return progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run remotely and display progress locally\n",
    "\n",
    "Now let's run the training code remotely.\n",
    "\n",
    "If we only cared about the final result, or if we were happy just printing progress to stdout, we could call `train` synchronously. But by calling it asynchronously, we can chart the metrics _while it runs_. The progress function is wrapped in a context manager that encapsulates communication with a `modal.Queue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_eggs.comms import send_to\n",
    "\n",
    "async with app.run(), send_to(Progress()) as emit_metrics:\n",
    "    model = await train.remote.aio(5, emit_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model function was created remotely, serialized, and sent back. Now we can run it locally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"What is the meaning of life?\"\n",
    "print(f\"Q: {x}\\nA: {model(x)}\")\n",
    "\n",
    "x = \"What is the airspeed velocity of an unladen swallow?\"\n",
    "print(f\"Q: {x}\\nA: {model(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
